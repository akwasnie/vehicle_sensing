{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Realtime_object_tracking.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/akwasnie/SiMa-GUT/blob/main/Realtime_object_tracking.ipynb)"
      ],
      "metadata": {
        "id": "5qIEJJfzbl_9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX3kUxdra7Ft"
      },
      "outputs": [],
      "source": [
        "# In Jupyter, you would need to install TF 2 via !pip.\n",
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "DV7RDA4r5gEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "from base64 import b64decode, b64encode\n",
        "\n",
        "import cv2\n",
        "import IPython\n",
        "import numpy as np\n",
        "import onnxruntime\n",
        "import torch\n",
        "from google.colab import output\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import Image, Javascript, clear_output, display\n",
        "from numpy import asarray\n",
        "from PIL import Image as pimage"
      ],
      "metadata": {
        "id": "nRHtTnEl5Ae2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/akwasnie/CenterNet.git && cd CenterNet && git checkout 8ef87b433529ac8f8bd4f95707f6bc05052c55e9"
      ],
      "metadata": {
        "id": "6MNuo7X05RCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('CenterNet')\n",
        "from CenterNet import src\n",
        "from src.lib.utils.image import get_affine_transform\n",
        "from src.lib.models.decode import ctdet_decode\n",
        "from src.lib.utils.post_process import ctdet_post_process"
      ],
      "metadata": {
        "id": "U72ukvG61KoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = 'data/'\n",
        "MODEL_FILE = os.path.join(DATA_DIR, 'ctdet_coco_dlav0_1x.onnx')\n",
        "CLASS_FILE = os.path.join(DATA_DIR, 'coco.json')\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "if not os.path.exists(MODEL_FILE):\n",
        "    !gdown --id 1Nda64Ezeo1yObABzDpJNzr-3n1yn_VOa -O $MODEL_FILE\n",
        "else:\n",
        "    print('CSV file ({}) already exists.'.format(MODEL_FILE))\n",
        "\n",
        "if not os.path.exists(CLASS_FILE):\n",
        "    !gdown --id 1ddXo-vbPNvNRs4X-faUkcwBtaWSP7ZtO -O $CLASS_FILE\n",
        "else:\n",
        "    print('CSV file ({}) already exists.'.format(CLASS_FILE))"
      ],
      "metadata": {
        "id": "v4audkP73Odr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IN_SIZE = [512,512]\n",
        "MEAN = [0.408, 0.447, 0.470]\n",
        "STD = [0.289, 0.274, 0.278]\n",
        "MAX_BB_NUM = 10\n",
        "THRESHOLD = 0.25\n",
        "FILENAME = 'photo.jpg'"
      ],
      "metadata": {
        "id": "SiK1Iz6G5yig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(image_path):\n",
        "  image = cv2.imread(image_path)\n",
        "  frame = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  h, w = frame.shape[:2]\n",
        "  c = [np.array([w/2., h/2.], dtype=np.float32)]\n",
        "  s = [max(h, w) * 1.0]\n",
        "  trans_input = get_affine_transform(c[0], s[0], 0, IN_SIZE)\n",
        "  im_data = cv2.warpAffine(\n",
        "      frame, trans_input,\n",
        "      (IN_SIZE[0], IN_SIZE[1]),\n",
        "      flags=cv2.INTER_LINEAR,\n",
        "  )\n",
        "  im_data = ((im_data / 255. - MEAN) / STD).astype(np.float32)\n",
        "  im_data = im_data.transpose(2, 0, 1).reshape(1, 3, IN_SIZE[0], IN_SIZE[1])\n",
        "  return frame, im_data, c, s\n",
        "\n",
        "def import_data(model_path, class_mapping_path):\n",
        "  ort_session = onnxruntime.InferenceSession(model_path)\n",
        "\n",
        "  with open(class_mapping_path, 'r') as fp:\n",
        "    category_index = json.load(fp)\n",
        "  return ort_session, category_index\n",
        "\n",
        "def centernet2cocodict(out, c, m, category_index):\n",
        "    hm = torch.from_numpy(out[0])\n",
        "    wh = torch.from_numpy(out[2])\n",
        "    reg = torch.from_numpy(out[1])\n",
        "    hm = hm.sigmoid_()\n",
        "    dets = ctdet_decode(hm, wh, reg=reg, cat_spec_wh=False, K=MAX_BB_NUM)\n",
        "    dets = dets.detach().cpu().numpy()\n",
        "    dets = ctdet_post_process(dets.copy(), c, m, 128, 128, 80)[0]\n",
        "\n",
        "    results = {}  \n",
        "    for j in range(1, 80 + 1):\n",
        "        results[j] = np.array(dets[j], dtype=np.float32).reshape(-1, 5)\n",
        "    \n",
        "    scores = np.hstack([results[j][:, 4] for j in range(1, 80 + 1)])\n",
        "\n",
        "    out_dict = {\n",
        "        'num_detections': MAX_BB_NUM,\n",
        "        'detection_boxes': [],\n",
        "        'detection_scores': [],\n",
        "        'detection_classes': [],\n",
        "        }\n",
        "\n",
        "    cats = list(category_index.values())\n",
        "    cats.sort(key=lambda x: x['id'])\n",
        "\n",
        "    for j in range(1, 80 + 1):\n",
        "        for bbox in results[j]:\n",
        "            if bbox[4] > THRESHOLD:\n",
        "                out_dict['detection_boxes'].append(bbox[:4])\n",
        "                out_dict['detection_scores'].append(bbox[4])\n",
        "                out_dict['detection_classes'].append(cats[j-1]['id'])\n",
        "\n",
        "    out_dict['detection_boxes'] = np.asarray(out_dict['detection_boxes'])\n",
        "    out_dict['detection_scores'] = np.asarray(out_dict['detection_scores'])\n",
        "    out_dict['detection_classes'] = np.asarray(out_dict['detection_classes'])\n",
        "\n",
        "    return out_dict\n",
        "\n",
        "def run_inference(ort_session, category_index, image_data, c, s):\n",
        "  output = ort_session.run(None, {ort_session.get_inputs()[0].name: image_data})\n",
        "  output_dict = centernet2cocodict(output, c, s, category_index)\n",
        "  return output_dict\n",
        "\n",
        "ort_session, category_index = import_data(MODEL_FILE, CLASS_FILE)"
      ],
      "metadata": {
        "id": "UFdHZx5h4ARR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O $FILENAME https://m.media-amazon.com/images/I/71lkKY9oGWL._AC_SX450_.jpg"
      ],
      "metadata": {
        "id": "PtzP0_y_J78x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame, image_data, c, s = preprocess(FILENAME)\n",
        "ort_session, category_index = import_data(MODEL_FILE, CLASS_FILE)\n",
        "output_dict = run_inference(ort_session, category_index, image_data, c, s)\n",
        "print(output_dict)\n",
        "for i in range(len(output_dict['detection_boxes'])):\n",
        "  bbox = output_dict['detection_boxes'][i]\n",
        "  class_name = category_index[str(output_dict['detection_classes'][i])]['name']\n",
        "  cv2.rectangle(frame,\n",
        "                (int(bbox[0]), int(bbox[1])),\n",
        "                (int(bbox[2]), int(bbox[3])),\n",
        "                (0,255,0),\n",
        "                2)\n",
        "  cv2.putText(frame,\n",
        "              class_name,\n",
        "              (int(bbox[0]), int(bbox[1]-10)),\n",
        "              0,\n",
        "              0.45,\n",
        "              (255,0,0),\n",
        "              0)\n",
        "result = np.asarray(frame)\n",
        "result = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "cv2_imshow(result)"
      ],
      "metadata": {
        "id": "cIUg1hfC957b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ndarray_to_b64(ndarray):\n",
        "    # Converts a np ndarray to a b64 string readable by html-img tags. \n",
        "    img = cv2.cvtColor(ndarray, cv2.COLOR_RGB2BGR)\n",
        "    _, buffer = cv2.imencode('.png', img)\n",
        "    return b64encode(buffer).decode('utf-8')\n",
        "\n",
        "def predict(img_64):\n",
        "  binary = b64decode(img_64.split(',')[1])\n",
        "  with open('photo.jpg', 'wb') as f:\n",
        "    f.write(binary)\n",
        "\n",
        "  frame, image_data, c, s = preprocess('photo.jpg')\n",
        "  out_dict = run_inference(ort_session, category_index, image_data, c, s)\n",
        "  print(out_dict)\n",
        "\n",
        "  for i in range(len(out_dict['detection_boxes'])):\n",
        "    bbox = out_dict['detection_boxes'][i]\n",
        "    class_name = category_index[str(out_dict['detection_classes'][i])]['name']\n",
        "    cv2.rectangle(frame,\n",
        "                  (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])),\n",
        "                  (0, 255, 0),\n",
        "                  2)\n",
        "    cv2.putText(frame,\n",
        "                class_name,\n",
        "                (int(bbox[0]), int(bbox[1]-10)),\n",
        "                0,\n",
        "                0.45,\n",
        "                (0, 0, 0),\n",
        "                0)\n",
        "  data = np.asarray(frame)\n",
        "  result = 'data:image/jpeg;base64,' + ndarray_to_b64(data)\n",
        "  return IPython.display.JSON({'result': result})\n",
        "\n",
        "output.register_callback('amld.predict', predict)"
      ],
      "metadata": {
        "id": "aiMQxD-Ra_NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, user-scalable=no\">\n",
        "<button id=\"start\">start</button><button id=\"clear\">clear</button><br />\n",
        "<canvas width=\"320\" height=\"180\" id=\"canvas\" style=\"border:1px solid black\"></canvas><br />\n",
        "<video id=\"myVideo\" width=\"320\" height=\"180\"></video><br />\n",
        "<image id=\"image\"></image>\n",
        "<script>\n",
        "  let canvas = document.getElementById('canvas')\n",
        "  let output = document.getElementById('output')\n",
        "  let ctx = canvas.getContext('2d')\n",
        "  let img_64\n",
        "  let dragging = false\n",
        "  let timeout\n",
        "  let stream \n",
        "  let video = document.getElementById('myVideo')\n",
        "\n",
        "\n",
        "  let predict = () => {\n",
        "    google.colab.kernel.invokeFunction('amld.predict', [img_64], {}).then(\n",
        "        obj => document.getElementById('image').src = obj.data['application/json'].result)\n",
        "  }\n",
        "\n",
        "  async function startvideo(){\n",
        "    stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: false })\n",
        "    video.srcObject = stream\n",
        "    await video.play();\n",
        "  }\n",
        "  \n",
        "  async function sendforprediction() {\n",
        "    ctx.drawImage(video, 0, 0, canvas.width, canvas.height)\n",
        "   \timg_64 = canvas.toDataURL('image/jpeg', 0.9)\n",
        "    clearTimeout(timeout)\n",
        "    timeout = setTimeout(predict, 500)\n",
        "  }\n",
        "\n",
        "  const handler = e => {\n",
        "    sendforprediction()\n",
        "  }\n",
        "  canvas.addEventListener('touchstart', e => {dragging=true; handler(e)})\n",
        "  canvas.addEventListener('touchmove', e => {e.preventDefault(); dragging && handler(e)})\n",
        "  canvas.addEventListener('touchend', () => dragging=false)\n",
        "  canvas.addEventListener('mousedown', e => {dragging=true; handler(e)})\n",
        "  canvas.addEventListener('mousemove', e => {dragging && handler(e)})\n",
        "  canvas.addEventListener('mouseup', () => dragging=false)\n",
        "  canvas.addEventListener('mouseleave', () => dragging=false)\n",
        "  document.getElementById('clear').addEventListener('click', () => {\n",
        "    ctx.fillStyle = 'white'\n",
        "    ctx.fillRect(0, 0, 320, 180)\n",
        "    stream.getTracks().forEach(function(track) {\n",
        "        track.stop();\n",
        "    });\n",
        "    clear_output();\n",
        "    video.srcObject = null;\n",
        "  })\n",
        "  document.getElementById('start').addEventListener('click', () => {\n",
        "    startvideo()\n",
        "  })\n",
        "</script>"
      ],
      "metadata": {
        "id": "uAaoeWhIbOgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EYlTtRCNn0e2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}